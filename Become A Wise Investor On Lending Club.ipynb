{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Become A Wise Investor on Lending Club\n",
    "\n",
    "LendingClub is a US peer-to-peer lending company, headquartered in San Francisco, California. It was the first peer-to-peer lender to register its offerings as securities with the Securities and Exchange Commission (SEC), and to offer loan trading on a secondary market. LendingClub is the world's largest peer-to-peer lending platform.\n",
    "\n",
    "Given historical data on loans given out with information on whether or not the borrower defaulted (charge-off), I will build a model that can predict wether or nor a borrower will pay back their loan. This way in the future when there is a new potential customer I can assess whether or not they are likely to pay back the loan. The datset can be obtained from [Kaggle](https://www.kaggle.com/wordsforthewise/lending-club)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('accepted_2007_to_2018Q4.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove redundant and leak information\n",
    "df.drop([\"desc\",\"url\",\"id\",\"member_id\",\"funded_amnt\",\"funded_amnt_inv\",\n",
    "         \"grade\",\"sub_grade\",\"emp_title\",\"issue_d\", \"zip_code\", \"out_prncp\", \n",
    "         \"out_prncp_inv\", \"total_pymnt\", \"total_pymnt_inv\", \"total_rec_prncp\",\n",
    "         \"total_rec_int\", \"total_rec_late_fee\", \"recoveries\", \n",
    "         \"collection_recovery_fee\", \"last_pymnt_d\", \"last_pymnt_amnt\"],\n",
    "          inplace=True,axis=1)\n",
    "# Remove columns with more than 50% missing values\n",
    "df.dropna(axis=1,thresh=df.shape[0]*0.5,inplace=True)\n",
    "\n",
    "# Select features and labels\n",
    "features_labels=['loan_amnt', 'term', 'int_rate', 'installment', 'emp_length',\n",
    "       'home_ownership', 'annual_inc', 'verification_status','loan_status',\n",
    "       'pymnt_plan', 'purpose', 'title', 'addr_state', 'dti', 'delinq_2yrs',\n",
    "       'earliest_cr_line', 'inq_last_6mths', 'open_acc', 'pub_rec',\n",
    "       'revol_bal', 'revol_util', 'total_acc', 'initial_list_status',\n",
    "       'last_credit_pull_d', 'collections_12_mths_ex_med', 'policy_code',\n",
    "       'application_type', 'acc_now_delinq', 'chargeoff_within_12_mths',\n",
    "       'delinq_amnt', 'pub_rec_bankruptcies', 'tax_liens']\n",
    "\n",
    "# The column that directly describes if a loan was paid off on time\n",
    "labels='loan_status'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df[features_labels]\n",
    "\n",
    "# I can treat the problem as a binary classification one: 'Fully Paid' or 'Charged Off'\n",
    "df = df[(df['loan_status'] == \"Fully Paid\") | (df['loan_status'] == \"Charged Off\")]\n",
    "status_replace = {\"loan_status\" : {\"Fully Paid\": 1,\"Charged Off\": 0,}}\n",
    "df = df.replace(status_replace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns that contain one true unique value\n",
    "orig_columns = df.columns\n",
    "drop_columns = []\n",
    "for col in orig_columns:\n",
    "    col_series = df[col].dropna().unique()\n",
    "    if len(col_series) == 1:\n",
    "        drop_columns.append(col)\n",
    "df.drop(drop_columns, axis=1,inplace=True)\n",
    "\n",
    "# Drop columns that offer very little variablity \n",
    "df.drop(['delinq_amnt','acc_now_delinq','collections_12_mths_ex_med',\n",
    "         'chargeoff_within_12_mths','tax_liens','application_type'],axis=1,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows containing missing values \n",
    "df.dropna(axis=0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Exlore object column that contain text\n",
    "object_columns_df = df.select_dtypes(include=[\"object\"])\n",
    "print(object_columns_df.head())\n",
    "\n",
    "# Drop categorical columns that contain too many values\n",
    "\n",
    "df.drop('addr_state',axis=1,inplace=True)\n",
    "\n",
    "# Title and purpose columns are repeated information. Remove title column\n",
    "print(df[\"title\"].value_counts())\n",
    "print(df[\"purpose\"].value_counts())\n",
    "df.drop('title',axis=1,inplace=True)\n",
    "\n",
    "# Extract the year from time stamp features\n",
    "df['earliest_cr_year'] = df['earliest_cr_line'].apply(lambda date:int(date[-4:]))\n",
    "df = df.drop('earliest_cr_line',axis=1)\n",
    "df['last_credit_pull_year'] = df['last_credit_pull_d'].apply(lambda date:int(date[-4:]))\n",
    "df = df.drop('last_credit_pull_d',axis=1)\n",
    "\n",
    "# Convert emp_length column to numeric type data\n",
    "df['emp_length'] = df['emp_length'].str.replace(r'\\+*\\syears*','').str.replace('< 1','0').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical and dummy variables\n",
    "# Convert home_ownership to dummy variables, but replace NONE and ANY with OTHER\n",
    "df['home_ownership']=df['home_ownership'].replace(['NONE', 'ANY'], 'OTHER')\n",
    "dummies = pd.get_dummies(df['home_ownership'],drop_first=True)\n",
    "df = df.drop('home_ownership',axis=1)\n",
    "df = pd.concat([df,dummies],axis=1)\n",
    "\n",
    "# Convert other categorical variables to dummy variables\n",
    "dummies = pd.get_dummies(df[['verification_status','initial_list_status','purpose','term']],drop_first=True)\n",
    "df = df.drop(['verification_status','initial_list_status','purpose','term'],axis=1)\n",
    "df = pd.concat([df,dummies],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab a sample of the dataset\n",
    "df = df.sample(frac=0.1,random_state=101)\n",
    "\n",
    "features = df[df.columns.drop('loan_status')]\n",
    "target = df['loan_status']\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df[df.columns.drop(['loan_status'])]\n",
    "target = df['loan_status']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(max_iter=500)\n",
    "\n",
    "# 10-fold cross validation\n",
    "predictions = cross_val_predict(lr, features, target, cv=10)\n",
    "predictions = pd.Series(predictions)\n",
    "\n",
    "# False positives.\n",
    "fp_filter = (predictions == 1) & (df[\"loan_status\"] == 0)\n",
    "fp = len(predictions[fp_filter])\n",
    "\n",
    "# True positives.\n",
    "tp_filter = (predictions == 1) & (df[\"loan_status\"] == 1)\n",
    "tp = len(predictions[tp_filter])\n",
    "\n",
    "# False negatives.\n",
    "fn_filter = (predictions == 0) & (df[\"loan_status\"] == 1)\n",
    "fn = len(predictions[fn_filter])\n",
    "\n",
    "# True negatives\n",
    "tn_filter = (predictions == 0) & (df[\"loan_status\"] == 0)\n",
    "tn = len(predictions[tn_filter])\n",
    "# Rates\n",
    "tpr = tp  / (tp + fn)\n",
    "fpr = fp  / (fp + tn)\n",
    "\n",
    "print(tpr)\n",
    "print(fpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Account for imbalance in the classes\n",
    "lr = LogisticRegression(class_weight='balanced',max_iter=500)\n",
    "\n",
    "# 10-fold cross validation\n",
    "predictions = cross_val_predict(lr, features, target, cv=10)\n",
    "predictions = pd.Series(predictions)\n",
    "\n",
    "# False positives.\n",
    "fp_filter = (predictions == 1) & (df[\"loan_status\"] == 0)\n",
    "fp = len(predictions[fp_filter])\n",
    "\n",
    "# True positives.\n",
    "tp_filter = (predictions == 1) & (df[\"loan_status\"] == 1)\n",
    "tp = len(predictions[tp_filter])\n",
    "\n",
    "# False negatives.\n",
    "fn_filter = (predictions == 0) & (df[\"loan_status\"] == 1)\n",
    "fn = len(predictions[fn_filter])\n",
    "\n",
    "# True negatives\n",
    "tn_filter = (predictions == 0) & (df[\"loan_status\"] == 0)\n",
    "tn = len(predictions[tn_filter])\n",
    "\n",
    "# Rates\n",
    "tpr = tp  / (tp + fn)\n",
    "fpr = fp  / (fp + tn)\n",
    "\n",
    "print(tpr)\n",
    "print(fpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To improve FPR, impose a penalty of 10 for misclassifying a 0 and a penalty of 1 for misclassifying a 1\n",
    "penalty = {0: 10, 1: 1}\n",
    "\n",
    "lr = LogisticRegression(class_weight=penalty,max_iter=500)\n",
    "predictions = cross_val_predict(lr, features, target, cv=10)\n",
    "predictions = pd.Series(predictions)\n",
    "\n",
    "# False positives.\n",
    "fp_filter = (predictions == 1) & (df[\"loan_status\"] == 0)\n",
    "fp = len(predictions[fp_filter])\n",
    "\n",
    "# True positives.\n",
    "tp_filter = (predictions == 1) & (df[\"loan_status\"] == 1)\n",
    "tp = len(predictions[tp_filter])\n",
    "\n",
    "# False negatives.\n",
    "fn_filter = (predictions == 0) & (df[\"loan_status\"] == 1)\n",
    "fn = len(predictions[fn_filter])\n",
    "\n",
    "# True negatives\n",
    "tn_filter = (predictions == 0) & (df[\"loan_status\"] == 0)\n",
    "tn = len(predictions[tn_filter])\n",
    "\n",
    "# Rates\n",
    "tpr = tp / (tp + fn)\n",
    "fpr = fp / (fp + tn)\n",
    "\n",
    "print(tpr)\n",
    "print(fpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Account for the imbalance in the classes\n",
    "rf = RandomForestClassifier(class_weight=\"balanced\")\n",
    "\n",
    "# 10-fold cross validation\n",
    "predictions = cross_val_predict(rf, features, target, cv=10)\n",
    "predictions = pd.Series(predictions)\n",
    "\n",
    "# False positives.\n",
    "fp_filter = (predictions == 1) & (df[\"loan_status\"] == 0)\n",
    "fp = len(predictions[fp_filter])\n",
    "\n",
    "# True positives.`\n",
    "tp_filter = (predictions == 1) & (df[\"loan_status\"] == 1)\n",
    "tp = len(predictions[tp_filter])\n",
    "\n",
    "# False negatives.\n",
    "fn_filter = (predictions == 0) & (df[\"loan_status\"] == 1)\n",
    "fn = len(predictions[fn_filter])\n",
    "\n",
    "# True negatives\n",
    "tn_filter = (predictions == 0) & (df[\"loan_status\"] == 0)\n",
    "tn = len(predictions[tn_filter])\n",
    "\n",
    "# Rates\n",
    "tpr = tp / (tp + fn)\n",
    "fpr = fp / (fp + tn)\n",
    "\n",
    "print(tpr)\n",
    "print(fpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation,Dropout\n",
    "from tensorflow.keras.constraints import max_norm\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# input layer\n",
    "model.add(Dense(36,  activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# hidden layer\n",
    "model.add(Dense(18, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# hidden layer\n",
    "model.add(Dense(9, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# output layer\n",
    "model.add(Dense(units=1,activation='sigmoid'))\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kf=KFold(n_splits=10, shuffle=True,random_state=1)\n",
    "fp=0\n",
    "tp=0\n",
    "fn=0\n",
    "tn=0\n",
    "\n",
    "for train_index,test_index in kf.split(features):\n",
    "    X_train,X_test = features.iloc[train_index].values, features.iloc[test_index].values\n",
    "    y_train,y_test = target.iloc[train_index].values, target.iloc[test_index].values\n",
    "    \n",
    "    # Data Transformation\n",
    "    scaler = MinMaxScaler()\n",
    "    X_train=scaler.fit_transform(X_train)\n",
    "    X_test=scaler.transform(X_test)\n",
    "    \n",
    "    \n",
    "    model.fit(x=X_train, y=y_train, epochs=100,batch_size=250)\n",
    "    predictions = model.predict_classes(X_test)\n",
    "    results=confusion_matrix(y_test,predictions)\n",
    "    \n",
    "    # False positives.\n",
    "    fp += results[0][1]\n",
    "\n",
    "    # True positives.`\n",
    "    tp += results[1][1]\n",
    "\n",
    "    # False negatives.\n",
    "    fn += results[1][0]\n",
    "    \n",
    "    # True negatives\n",
    "    tn += results[0][0]\n",
    "\n",
    "# Rates\n",
    "tpr = tp / (tp + fn)\n",
    "fpr = fp / (fp + tn)\n",
    "print(tpr)\n",
    "print(fpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Without accounting for imbalanced classes, the 3 models are good at identifying all the good loans (true positive rate), but also incorrectly identify most of bad loans (false positive rate). Neural networks model proves to be better (lowest FPR).\n",
    "\n",
    "When I account for imbalanced classes, the logistic model can lower the FPR to 5.2%, and thus lower the risk. Note that this comes at the expense of true positive rate (19.2%)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
